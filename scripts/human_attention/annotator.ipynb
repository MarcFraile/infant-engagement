{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Attention Annotation: Painting by Hand\n",
    "\n",
    "## Sample Selection\n",
    "\n",
    "Each annotator has up to 12 5-second snippets to annotate. These have been selected from videos that are:\n",
    "\n",
    "1. In the test fold for the learning algorithm, and\n",
    "2. In the unseen fold for the corresponding annotator.\n",
    "\n",
    "The snippets have been chosen for each task (people, eggs, drums) and each variable (attending, participating), choosing when possible two samples that the remaining annotators agreed on.\n",
    "\n",
    "Because of annotator disagreements in the original ELAN phase, not every annotator will have to cover 12 samples in this painting phase. Just work through the examples marked with your initials.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Select a sample marked with your initials from the drop-down menu.\n",
    "2. Watch the 5s snippet (use the play button).\n",
    "3. Classify the sample using the label buttons.\n",
    "4. Paint on the video using your mouse to select the most important moments and image details for your decision (see **Controls** section below).\n",
    "5. **DON'T FORGET TO SAVE!**\n",
    "6. Repeat until all videos are annotated.\n",
    "7. Send the data to Marc, as usual.\n",
    "\n",
    "## Controls\n",
    "\n",
    "* **Primary button press:**   Add attention to an area you deemed important (doesn't accumulate if the mouse is static! move your mouse slightly to increase intensity).\n",
    "* **Secondary button press:** Delete attention from an area (like with the primary button, you need to move to keep erasing).\n",
    "* **Mouse wheel:**            Change the size of the \"paint brush\" (represented by the in-screen circle).\n",
    "\n",
    "## Attention Target\n",
    "\n",
    "The learning algorithms have been trained on *binarized* versions of our annotations (they just consider if the child is participating at all vs. not participating, or equivalently if the child is attending at all vs. not attending). From the completely trained networks, we use *attention* algorithms to mark different parts of the video as more or less relevant for the final decision.\n",
    "\n",
    "Our painted annotations are a human baseline to compare against the attention algorithm. Hence, we should mark any moments and/or frame regions that we consider important to determine (for us as humans who have some expertise with this dataset) if the child is attending (resp. participating) or not.\n",
    "\n",
    "In case of doubt, look at the sample information in the drop-down menu to determine which variable you should paint attention for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "from local.attention_painting import Annotator\n",
    "from local.navigation import get_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo Root  (absolute)         : /home/marcfraile/Documents/PhD/self-study/infant-engagement\n",
      "Script Dir (relative to repo) : scripts/human_attention\n"
     ]
    }
   ],
   "source": [
    "script_dir = Path(os.getcwd())\n",
    "repo_root = get_repo_root(script_dir)\n",
    "os.chdir(repo_root)\n",
    "\n",
    "print(f\"Repo Root  (absolute)         : {repo_root}\")\n",
    "print(f\"Script Dir (relative to repo) : {script_dir.relative_to(repo_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPPET_DURATION : float =  5.0 # seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_root        = Path(\"data/processed/video/\")\n",
    "annotation_root = Path(\"data/processed/human_attention/\")\n",
    "snippet_file    = Path(\"data/processed/human_attention/candidate_snippets.csv\")\n",
    "\n",
    "assert vid_root.is_dir()\n",
    "assert annotation_root.is_dir()\n",
    "assert snippet_file.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Attention Example\n",
    "\n",
    "Below you can see an example of a machine attention algorithm (*guided grad-CAM*) targeting a positive \"participating\" example (which was correctly classified as positive). This can give us an approximate reference of how much and how thick we should paint attention wherever we consider it reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url=\"machine_attention.mp4\", width=160*3, height=160*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Attention Example\n",
    "\n",
    "Below you can see an example of a painted annotation by me. The target here was \"attending\". According to what we discussed in the ELAN annotation phase, I considered the fact that the child was participating (arm movement) as secondary proof (not very intense). The gaze from the child to the experimenter towards the end was seen as strong evidence of attention.\n",
    "\n",
    "For this, I used the default brush size, but feel free to adjust it to your liking. I also had some amount of painting on every frame, but it might make sense to leave most frames empty (more similar to the machine example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url=\"human_attention.mp4\", width=160*3, height=160*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Tool\n",
    "\n",
    "Time to paint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ded23e0b8154a8c863241ddba58b1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Canvas(capture_scroll=True, footer_visible=False, header_visible=False, layout=Lâ€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator = Annotator(SNIPPET_DURATION, vid_root, annotation_root, snippet_file)\n",
    "annotator.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "afba8fa7da6b510cfa5daabc9353f551809708a71778ea44deeccff051479d97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
